{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../output/human_ai_study/\"\n",
    "ppt_data_paths = glob.glob(f\"{root_path}*\")\n",
    "ppt_data_paths.sort(key=lambda x: x.split(\"/\")[-1].split(\"_\")[-1])\n",
    "ppt_ids = [x.split('/')[-1] for x in ppt_data_paths]\n",
    "ppt_data_paths = list(zip(ppt_ids, ppt_data_paths))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"1st_session\": [\"pendulum\", \"rdk_alone_first\", \"rdk_alone_AI_assist_first\", \"rdk_alone_hitl\"],\n",
    "    \"2nd_session\": [\"rdk_alone_second\", \"rdk_alone_AI_assist_second_original\", \"rdk_alone_AI_assist_second_retrained\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "returns the average subarray size where continuous inputs are non-zero\n",
    "'''\n",
    "def average_nonzero_subarray_size(arr):\n",
    "    nonzero_subarray_sizes = []\n",
    "    current_subarray_size = 0\n",
    "\n",
    "    for value in arr:\n",
    "        if value != 0:\n",
    "            # Inside a contiguous subarray with non-zero values\n",
    "            current_subarray_size += 1\n",
    "        elif value == 0 and current_subarray_size > 0:\n",
    "            # End of a contiguous subarray with non-zero values\n",
    "            nonzero_subarray_sizes.append(current_subarray_size)\n",
    "            current_subarray_size = 0\n",
    "\n",
    "    if current_subarray_size > 0:\n",
    "        nonzero_subarray_sizes.append(current_subarray_size)\n",
    "\n",
    "    if nonzero_subarray_sizes:\n",
    "        return sum(nonzero_subarray_sizes) / len(nonzero_subarray_sizes)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "'''\n",
    "returns the average subarray size where continuous inputs are zero\n",
    "'''\n",
    "def average_zero_subarray_size(arr):\n",
    "    interval_sizes = []\n",
    "    current_interval_size = 0\n",
    "\n",
    "    for value in arr:\n",
    "        if value == 0 and current_interval_size >= 0:\n",
    "            # Inside an interval\n",
    "            current_interval_size += 1\n",
    "        elif value != 0 and current_interval_size > 0:\n",
    "            # End of an interval\n",
    "            interval_sizes.append(current_interval_size)\n",
    "            current_interval_size = 0\n",
    "\n",
    "    if current_interval_size > 0:\n",
    "        interval_sizes.append(current_interval_size)\n",
    "\n",
    "    if interval_sizes:\n",
    "        return sum(interval_sizes) / len(interval_sizes)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df, index, isCatch):\n",
    "    times = np.array(df['time'])\n",
    "    num_actions = np.size(df[\"destabilizing_actions\"])\n",
    "    num_destabilizing_actions = np.sum(df[\"destabilizing_actions\"])\n",
    "    destabilizing_prop = num_destabilizing_actions/num_actions\n",
    "    num_crash_cond_triggers = np.sum(df[\"is_crash_condition_triggered\"])\n",
    "    crash_cond_triggered_prop = num_crash_cond_triggers / num_actions\n",
    "    n_crashes = np.sum(np.array(df[\"action_made_by\"]) == 0.0)-1\n",
    "    last_timestamp = times[-1]\n",
    "    crash_freq = n_crashes/last_timestamp\n",
    "    avg_crash_probability = np.mean(np.array(df[\"crash_probabilities\"]))\n",
    "    avg_dob_dist = np.mean(np.abs(np.array(df[\"angular position\"])))\n",
    "    sd_angular_pos = np.std(np.array(df[\"angular position\"]))\n",
    "    avg_angvel_mag = np.mean(np.abs(np.array(df[\"angular velocity\"])))\n",
    "    sd_angvel = np.std(np.array(df[\"angular velocity\"]))\n",
    "    angvel_rms = np.sqrt(np.mean(np.array(df[\"angular velocity\"])**2))\n",
    "    avg_defl_mag = np.mean(np.abs(np.array(df[\"joystick deflection\"])))\n",
    "    frequency_data = len(times)/last_timestamp\n",
    "    average_cont_non_zero_input = average_nonzero_subarray_size(np.array(df[\"joystick deflection\"]))\n",
    "    prop_anticipatory_deflections = np.sum(df['anticipatory_deflections']) / num_actions\n",
    "    average_cont_zero_input = average_zero_subarray_size(np.array(df[\"joystick deflection\"]))\n",
    "    avg_dur_cont_non_zero_input = average_cont_non_zero_input/frequency_data\n",
    "    avg_dur_cont_zero_input = average_cont_zero_input/frequency_data\n",
    "\n",
    "    return pd.DataFrame( [{\n",
    "        \"trial_num\": index,\n",
    "        \"isCatch\": isCatch,\n",
    "        \"perc_destab_actions\": destabilizing_prop*100,\n",
    "        \"perc_anticip_actions\": prop_anticipatory_deflections * 100,\n",
    "        \"num_crashes\": n_crashes,\n",
    "        \"average_crash_prob\": avg_crash_probability,\n",
    "        \"crash_freqs\": crash_freq,\n",
    "        \"mean_dist_dob\": avg_dob_dist,\n",
    "        \"ang_pos_sd\": sd_angular_pos,\n",
    "        \"vel_mag_mean\": avg_angvel_mag,\n",
    "        \"ang_vel_sd\": sd_angvel,\n",
    "        \"vel_rms\": angvel_rms,\n",
    "        \"defl_mag_mean\": avg_defl_mag,\n",
    "        \"avg_dur_cont_non_zero_inputs\": avg_dur_cont_non_zero_input,\n",
    "        \"avg_dur_cont_zero_inputs\": avg_dur_cont_zero_input,\n",
    "        \"perc_crash_cond_triggered\": crash_cond_triggered_prop*100,\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "def plot_graphs(df, ppt_id, ppt_path, session, task, dp):\n",
    "\n",
    "    # make position-velocity plots\n",
    "    for i, x in enumerate([(\"other\", \"black\"), (\"destab\", \"red\"), (\"anticip\", \"blue\"),]):\n",
    "        label, color = x\n",
    "        plt.scatter(df[df[\"deflection_type_label\"] == i][\"angular position\"], df[df[\"deflection_type_label\"] == i][\"angular velocity\"], marker=\".\", c=color, label=label, alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Angular Position (θ)\")\n",
    "    plt.ylabel(\"Angular Velocity (θ/s)\")\n",
    "    plt.xlim((-62, 62))\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{'/'.join(dp.split('/')[:-1])}/position_velocity_plot.png\", dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "COLUMNS = ['task', 'perc_destab_actions', 'perc_anticip_actions', 'num_crashes',\n",
    "       'average_crash_prob', 'crash_freqs', 'mean_dist_dob', 'ang_pos_sd',\n",
    "       'vel_mag_mean', 'ang_vel_sd', 'vel_rms', 'defl_mag_mean',\n",
    "       'avg_dur_cont_non_zero_inputs', 'avg_dur_cont_zero_inputs',\n",
    "       'perc_crash_cond_triggered', ]\n",
    "\n",
    "def process_data(ppt_id, ppt_path, session):\n",
    "    print(f\"processing {ppt_id}\")\n",
    "\n",
    "    # for each plot the position-velocity phase plots,\n",
    "    # summarize performance for each task.\n",
    "    participant_summaries = []\n",
    "    for task in tasks[session]:\n",
    "        print(f\"    {task}\")\n",
    "        curr_path = f\"{ppt_path}/{task}/\"\n",
    "        data_paths = glob.glob(f\"{curr_path}*/*.csv\")\n",
    "        catch_trial = glob.glob(f\"{curr_path}*/*.json\")\n",
    "        toSkip = set()\n",
    "        if len(catch_trial):\n",
    "            subpath = \"/\".join(catch_trial[0].split('/')[:-1])\n",
    "            for p in data_paths:\n",
    "                if subpath in p:\n",
    "                    toSkip.add(p)\n",
    "        # elif session==\"1st_session\" and task == \"rdk_alone_AI_assist_first\":\n",
    "        #     toSkip.add(data_paths[0])\n",
    "\n",
    "        task_summaries = []\n",
    "        total_summary = {\n",
    "            \"trial_num\": \"summary\",\n",
    "            \"isCatch\": \"summary\",\n",
    "            \"perc_destab_actions\": 0,\n",
    "            \"perc_anticip_actions\": 0,\n",
    "            \"num_crashes\": 0,\n",
    "            \"crash_freqs\": 0,\n",
    "            \"mean_dist_dob\": 0,\n",
    "            \"average_crash_prob\": 0,\n",
    "            \"ang_pos_sd\": 0,\n",
    "            \"vel_mag_mean\": 0,\n",
    "            \"ang_vel_sd\": 0,\n",
    "            \"vel_rms\": 0,\n",
    "            \"defl_mag_mean\": 0,\n",
    "            \"avg_dur_cont_non_zero_inputs\": 0,\n",
    "            \"avg_dur_cont_zero_inputs\": 0,\n",
    "            \"perc_crash_cond_triggered\": 0,\n",
    "        }\n",
    "        for i, dp in enumerate(data_paths):\n",
    "            df = pd.read_csv(dp)\n",
    "\n",
    "            # adding more columns for certain features \n",
    "            check_anticipatory_deflection = lambda x: 1 if x[0]!=0 and np.sign(x[1])!=np.sign(x[2])  else 0\n",
    "            df['anticipatory_deflections'] = df.apply(lambda x: check_anticipatory_deflection([x[1], x[2], x[3]]), axis=1)\n",
    "            assign_label = lambda x,y: 0 if (x == y) or (not x and not y) else 1 if x == 1 else 2\n",
    "            df['deflection_type_label'] = df.apply(lambda x: assign_label(x[7], x[10]), axis=1)\n",
    "            \n",
    "            plot_graphs(df, ppt_id, ppt_path, session, task, dp)\n",
    "             \n",
    "            summary = summarize_dataframe(df, i, 1 if dp in toSkip else 0)\n",
    "            task_summaries.append(summary)\n",
    "            if dp not in toSkip:\n",
    "                for k in total_summary:\n",
    "                    if k in [\"trial_num\", \"isCatch\"]:\n",
    "                        continue\n",
    "                    total_summary[k] += summary[k].iloc[0]\n",
    "        if len(data_paths):\n",
    "            for k in total_summary:\n",
    "                if k in [\"trial_num\", \"isCatch\", \"num_crashes\"]:\n",
    "                    continue\n",
    "                total_summary[k] /= (len(data_paths) - len(toSkip))\n",
    "\n",
    "\n",
    "            print(f\"        {len(task_summaries)}\")\n",
    "            task_summaries.append(pd.DataFrame([total_summary]))\n",
    "            summary_df = pd.concat(task_summaries)\n",
    "\n",
    "            summary_df.to_csv(f\"{ppt_path}/{task}_summary.csv\", index=False)\n",
    "\n",
    "            summary_df[\"task\"] = summary_df[\"trial_num\"].apply(lambda _: task)\n",
    "\n",
    "            participant_summaries.append(summary_df[summary_df[\"trial_num\"] == \"summary\"].drop(columns=[\"trial_num\", \"isCatch\"]))\n",
    "    \n",
    "    if len(participant_summaries):\n",
    "        print(f\"    {len(participant_summaries)}\")\n",
    "        ppt_summary_df = pd.concat(participant_summaries)\n",
    "        ppt_summary_df = ppt_summary_df[COLUMNS]\n",
    "        \n",
    "        ppt_summary_df.to_csv(f\"{ppt_path}/{session}_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ppt_id, ppt_path in ppt_data_paths:\n",
    "    process_data(ppt_id, ppt_path, \"1st_session\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ppt_id, ppt_path in ppt_data_paths:\n",
    "    process_data(ppt_id, ppt_path, \"2nd_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_model_mappings = pd.read_excel(\"../python_vip/participants_models_mappings.xlsx\").head(20)\n",
    "participant_model_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_shortHand = {\n",
    " 'num_crashes': '# Crashes',\n",
    " 'perc_destab_actions': '% destab.',\n",
    " 'perc_anticip_actions': '% anticip.',\n",
    " 'mean_dist_dob': 'μ|θ|',\n",
    " 'ang_pos_sd': 'σ(θ)',\n",
    " 'vel_mag_mean': 'μ|Mag|vel',\n",
    " 'Angular velocity SD': 'σ(|Mag|vel)',\n",
    " 'vel_rms': 'vel RMS',\n",
    " 'Deflection magnitude mean': 'μ|d| * 30'\n",
    "}\n",
    "\n",
    "\n",
    "COLUMNS =  [ 'perc_destab_actions', 'perc_anticip_actions', 'num_crashes',\n",
    "       'average_crash_prob', 'crash_freqs', 'mean_dist_dob', 'ang_pos_sd',\n",
    "       'vel_mag_mean', 'ang_vel_sd', 'vel_rms', 'defl_mag_mean',\n",
    "       'avg_dur_cont_non_zero_inputs', 'avg_dur_cont_zero_inputs',\n",
    "        ]\n",
    "\n",
    "\n",
    "assistant_map = {\n",
    "    \"A\": \"SAC\",\n",
    "    \"B\": \"SAC-AIRL\",\n",
    "    \"C\": \"DDPG\",\n",
    "    \"D\": \"MLP\",\n",
    "    \"E\": \"LSTM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First session RDK ALone vs AI assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "COLUMNS_PLOT = ['num_crashes','perc_destab_actions', 'perc_destab_actions' , 'mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in ppt_data_paths:\n",
    "    model_used_sess_1 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model\"].iloc[0]\n",
    "\n",
    "    model_used_sess_2 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model_second_session\"].iloc[0]\n",
    "    print(ppt_id, model_used_sess_1)\n",
    "\n",
    "    summary_1 = pd.read_csv(f\"{ppt_path}/1st_session_summary.csv\")\n",
    "\n",
    "\n",
    "    for col in COLUMNS_PLOT:\n",
    "        metric_diff = summary_1[summary_1.task == \"rdk_alone_AI_assist_first\"][col].values - summary_1[summary_1.task == \"rdk_alone_first\"][col].values\n",
    "        \n",
    "        metric_diff = metric_diff[0]\n",
    "        if metric_diff == np.inf:\n",
    "            metric_diff = 0\n",
    "        \n",
    "        \n",
    "        total_diff_data.append(\n",
    "            [ppt_id, assistant_map[model_used_sess_1], columns_shortHand[col], metric_diff] \n",
    "        )\n",
    "    \n",
    "\n",
    "total_diff_df1 = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second session RDK ALone vs AI assistance original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "COLUMNS_PLOT = ['num_crashes', 'perc_destab_actions', 'mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in ppt_data_paths[::-1]:\n",
    "    model_used_sess_1 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model\"].iloc[0]\n",
    "\n",
    "    model_used_sess_2 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model_second_session\"].iloc[0]\n",
    "    print(ppt_id, model_used_sess_2)\n",
    "    try:\n",
    "        summary_1 = pd.read_csv(f\"{ppt_path}/2nd_session_summary.csv\")\n",
    "        \n",
    "        for col in COLUMNS_PLOT:\n",
    "            metric_diff = summary_1[summary_1.task == \"rdk_alone_AI_assist_second_original\"][col].values - summary_1[summary_1.task == \"rdk_alone_second\"][col].values\n",
    "            metric_diff = metric_diff[0]\n",
    "            if metric_diff == np.inf:\n",
    "                metric_diff = 0\n",
    "            \n",
    "            \n",
    "            total_diff_data.append(\n",
    "                [ppt_id, assistant_map[model_used_sess_2], columns_shortHand[col], metric_diff] \n",
    "            )\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "total_diff_df2 = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second session RDK ALone vs AI assistance retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "COLUMNS_PLOT = ['num_crashes', 'perc_destab_actions', 'mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in ppt_data_paths[::-1]:\n",
    "    model_used_sess_1 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model\"].iloc[0]\n",
    "\n",
    "    model_used_sess_2 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model_second_session\"].iloc[0]\n",
    "    print(ppt_id, model_used_sess_2)\n",
    "\n",
    "    try:\n",
    "        summary_1 = pd.read_csv(f\"{ppt_path}/2nd_session_summary.csv\")\n",
    "\n",
    "\n",
    "        for col in COLUMNS_PLOT:\n",
    "            metric_diff = summary_1[summary_1.task == \"rdk_alone_AI_assist_second_retrained\"][col].values - summary_1[summary_1.task == \"rdk_alone_second\"][col].values\n",
    "            # print(metric_diff)\n",
    "            metric_diff = metric_diff[0]\n",
    "            if metric_diff == np.inf:\n",
    "                metric_diff = 0\n",
    "            \n",
    "            \n",
    "            total_diff_data.append(\n",
    "                [ppt_id, assistant_map[model_used_sess_2], columns_shortHand[col], metric_diff] \n",
    "            )\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "total_diff_df3 = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set(rc={'figure.figsize':(45,10)})\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True, sharex=True)\n",
    "\n",
    "sns.boxplot(x=\"feature\", y=\"diff\", hue=\"model\", data=total_diff_df1, ax=ax1)\n",
    "ax1.set(xlabel=\"\", ylabel='Absolute difference')\n",
    "ax1.get_legend().remove()\n",
    "# plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "sns.boxplot(x=\"feature\", y=\"diff\", hue=\"model\", data=total_diff_df2, ax=ax2)\n",
    "# ax2.set(xlabel='Task metrics')\n",
    "ax2.set_xlabel('Task metrics', labelpad=20)\n",
    "ax2.get_legend().remove()\n",
    "\n",
    "sns.boxplot(x=\"feature\", y=\"diff\", hue=\"model\", data=total_diff_df3, ax=ax3)\n",
    "ax3.set(xlabel='', ylabel='')\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.7))\n",
    "\n",
    "plt.ylim(-40, 60);\n",
    "\n",
    "plt.savefig(\"./results/human_ai_study/combined_first_second-original_second-retrained_differences.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "# sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second session alone difference vs first session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_PLOT = ['num_crashes', 'perc_destab_actions' , 'mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in ppt_data_paths:\n",
    "    model_used_sess_1 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model\"].iloc[0]\n",
    "\n",
    "    model_used_sess_2 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model_second_session\"].iloc[0]\n",
    "    print(ppt_id, model_used_sess_2)\n",
    "\n",
    "    try:\n",
    "        summary_1 = pd.read_csv(f\"{ppt_path}/2nd_session_summary.csv\")\n",
    "        \n",
    "        x = pd.read_csv(f\"{ppt_path}/1st_session_summary.csv\")\n",
    "        for col in COLUMNS_PLOT:\n",
    "            \n",
    "            total_diff_data.append(\n",
    "                [ppt_id, assistant_map[model_used_sess_1], columns_shortHand[col], x[x.task == \"rdk_alone_first\"][col].values[0], \"1st session\"] \n",
    "            )\n",
    "            total_diff_data.append(\n",
    "                [ppt_id, assistant_map[model_used_sess_1], columns_shortHand[col], summary_1[summary_1.task == \"rdk_alone_second\"][col].values[0], \"2nd session\"] \n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "total_diff_df = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\", \"session\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"feature\", y=\"diff\", hue=\"session\",\n",
    "            data=total_diff_df, )\n",
    "\n",
    "plt.xlabel(\"Task metrics\", labelpad=20)\n",
    "plt.ylabel(\"Average metric value over 3 trials\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig(f\"./results/human_ai_study/second_session_alone_difference_first_session.pdf\", dpi=300, bbox_inches=\"tight\");\n",
    "# sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot catch trial metrics for original and retrained models compared to last solo trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_PLOT = ['num_crashes', 'perc_destab_actions','mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in ppt_data_paths[::-1]:\n",
    "    print(ppt_id, )\n",
    "    solo = pd.read_csv(f\"{ppt_path}/rdk_alone_second_summary.csv\")\n",
    "    original = pd.read_csv(f\"{ppt_path}/rdk_alone_AI_assist_second_original_summary.csv\")\n",
    "    retrained = pd.read_csv(f\"{ppt_path}/rdk_alone_AI_assist_second_retrained_summary.csv\")\n",
    "    model_used_sess_2 = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model_second_session\"].iloc[0]\n",
    "    for col in COLUMNS_PLOT:\n",
    "\n",
    "        diff = original[original.isCatch == \"1\"][col].iloc[0] - solo[solo.trial_num == \"2\"][col].iloc[0]\n",
    "\n",
    "        if diff == np.inf:\n",
    "            diff = 0\n",
    "        \n",
    "        total_diff_data.append(\n",
    "            [ppt_id, assistant_map[model_used_sess_2], columns_shortHand[col], diff, \"original\"]\n",
    "        )\n",
    "\n",
    "        diff = retrained[retrained.isCatch == \"1\"][col].iloc[0] - solo[solo.trial_num == \"2\"][col].iloc[0]\n",
    "\n",
    "        if diff == np.inf:\n",
    "            diff = 0\n",
    "        total_diff_data.append(\n",
    "            [ppt_id, assistant_map[model_used_sess_2], columns_shortHand[col], diff, \"retrained\"]\n",
    "        )\n",
    "\n",
    "total_diff_df = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\", \"version\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"feature\", y=\"diff\",\n",
    "            hue=\"model\",\n",
    "            data=total_diff_df[total_diff_df.version == \"original\"],\n",
    "            )\n",
    "\n",
    "\n",
    "plt.xlabel(\"Task metrics\", labelpad=20)\n",
    "plt.ylabel(\"Absolute difference\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig(f\"./results/human_ai_study/second_session_ai_assist_catch_trial_original.pdf\", dpi=300, bbox_inches=\"tight\");\n",
    "# sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"feature\", y=\"diff\",\n",
    "            hue=\"model\",\n",
    "            data=total_diff_df[total_diff_df.version == \"retrained\"],\n",
    "            )\n",
    "\n",
    "plt.xlabel(\"Task metrics\", labelpad=20)\n",
    "plt.ylabel(\"Absolute difference\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig(f\"./results/human_ai_study/second_session_ai_assist_catch_trial_retrained.pdf\", dpi=300, bbox_inches=\"tight\");\n",
    "# sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df, index, isCatch):\n",
    "    times = np.array(df['time'])\n",
    "    num_actions = np.size(df[\"destabilizing_actions\"])\n",
    "    num_destabilizing_actions = np.sum(df[\"destabilizing_actions\"])\n",
    "    destabilizing_prop = num_destabilizing_actions/num_actions\n",
    "    num_crash_cond_triggers = np.sum(df[\"is_crash_condition_triggered\"])\n",
    "    crash_cond_triggered_prop = num_crash_cond_triggers / num_actions\n",
    "    n_crashes = np.sum(np.array(df[\"action_made_by\"]) == 0.0)-1\n",
    "    last_timestamp = times[-1]\n",
    "    crash_freq = n_crashes/last_timestamp\n",
    "    avg_crash_probability = np.mean(np.array(df[\"crash_probabilities\"]))\n",
    "    avg_dob_dist = np.mean(np.abs(np.array(df[\"angular position\"])))\n",
    "    sd_angular_pos = np.std(np.array(df[\"angular position\"]))\n",
    "    avg_angvel_mag = np.mean(np.abs(np.array(df[\"angular velocity\"])))\n",
    "    sd_angvel = np.std(np.array(df[\"angular velocity\"]))\n",
    "    angvel_rms = np.sqrt(np.mean(np.array(df[\"angular velocity\"])**2))\n",
    "    avg_defl_mag = np.mean(np.abs(np.array(df[\"joystick deflection\"])))\n",
    "    frequency_data = len(times)/last_timestamp\n",
    "    average_cont_non_zero_input = average_nonzero_subarray_size(np.array(df[\"joystick deflection\"]))\n",
    "    prop_anticipatory_deflections = np.sum(df['anticipatory_deflections']) / num_actions\n",
    "    average_cont_zero_input = average_zero_subarray_size(np.array(df[\"joystick deflection\"]))\n",
    "    avg_dur_cont_non_zero_input = average_cont_non_zero_input/frequency_data\n",
    "    avg_dur_cont_zero_input = average_cont_zero_input/frequency_data\n",
    "\n",
    "    return pd.DataFrame( [{\n",
    "        \"trial_num\": index,\n",
    "        \"isCatch\": isCatch,\n",
    "        \"perc_destab_actions\": destabilizing_prop*100,\n",
    "        \"perc_anticip_actions\": prop_anticipatory_deflections * 100,\n",
    "        \"num_crashes\": n_crashes,\n",
    "        \"average_crash_prob\": avg_crash_probability,\n",
    "        \"crash_freqs\": crash_freq,\n",
    "        \"mean_dist_dob\": avg_dob_dist,\n",
    "        \"ang_pos_sd\": sd_angular_pos,\n",
    "        \"vel_mag_mean\": avg_angvel_mag,\n",
    "        \"ang_vel_sd\": sd_angvel,\n",
    "        \"vel_rms\": angvel_rms,\n",
    "        \"defl_mag_mean\": avg_defl_mag,\n",
    "        \"avg_dur_cont_non_zero_inputs\": avg_dur_cont_non_zero_input,\n",
    "        \"avg_dur_cont_zero_inputs\": avg_dur_cont_zero_input,\n",
    "        \"perc_crash_cond_triggered\": crash_cond_triggered_prop*100,\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "def plot_graphs(df, model_name, model_path, session, task, dp):\n",
    "\n",
    "    # make position-velocity plots\n",
    "    for i, x in enumerate([(\"other\", \"black\"), (\"destab\", \"red\"), (\"anticip\", \"blue\"),]):\n",
    "        label, color = x\n",
    "        plt.scatter(df[df[\"deflection_type_label\"] == i][\"angular position\"], df[df[\"deflection_type_label\"] == i][\"angular velocity\"], marker=\".\", c=color, label=label, alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Angular Position (θ)\")\n",
    "    plt.ylabel(\"Angular Velocity (θ/s)\")\n",
    "    plt.xlim((-62, 62))\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{'/'.join(dp.split('/')[:-1])}/position_velocity_plot.png\", dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "COLUMNS = ['task', 'perc_destab_actions', 'perc_anticip_actions', 'num_crashes',\n",
    "       'average_crash_prob', 'crash_freqs', 'mean_dist_dob', 'ang_pos_sd',\n",
    "       'vel_mag_mean', 'ang_vel_sd', 'vel_rms', 'defl_mag_mean',\n",
    "       'avg_dur_cont_non_zero_inputs', 'avg_dur_cont_zero_inputs',\n",
    "       'perc_crash_cond_triggered', ]\n",
    "\n",
    "def process_data(model_name, model_path, session):\n",
    "    print(f\"processing {model_name}\")\n",
    "\n",
    "    # for each plot the position-velocity phase plots,\n",
    "    # summarize performance for each task.\n",
    "    model_summaries = []\n",
    "    for task in [\"original\", \"retrained\"]:\n",
    "        print(f\"    {task}\")\n",
    "        curr_path = f\"{model_path}/{task}/\"\n",
    "        data_paths = glob.glob(f\"{curr_path}*/*.csv\")\n",
    "\n",
    "        task_summaries = []\n",
    "        total_summary = {\n",
    "            \"trial_num\": \"summary\",\n",
    "            \"perc_destab_actions\": 0,\n",
    "            \"perc_anticip_actions\": 0,\n",
    "            \"num_crashes\": 0,\n",
    "            \"crash_freqs\": 0,\n",
    "            \"mean_dist_dob\": 0,\n",
    "            \"average_crash_prob\": 0,\n",
    "            \"ang_pos_sd\": 0,\n",
    "            \"vel_mag_mean\": 0,\n",
    "            \"ang_vel_sd\": 0,\n",
    "            \"vel_rms\": 0,\n",
    "            \"defl_mag_mean\": 0,\n",
    "            \"avg_dur_cont_non_zero_inputs\": 0,\n",
    "            \"avg_dur_cont_zero_inputs\": 0,\n",
    "            \"perc_crash_cond_triggered\": 0,\n",
    "        }\n",
    "        for i, dp in enumerate(data_paths):\n",
    "            df = pd.read_csv(dp)\n",
    "\n",
    "            # adding more columns for certain features \n",
    "            check_anticipatory_deflection = lambda x: 1 if x[0]!=0 and np.sign(x[1])!=np.sign(x[2])  else 0\n",
    "            df['anticipatory_deflections'] = df.apply(lambda x: check_anticipatory_deflection([x[1], x[2], x[3]]), axis=1)\n",
    "            assign_label = lambda x,y: 0 if (x == y) or (not x and not y) else 1 if x == 1 else 2\n",
    "            df['deflection_type_label'] = df.apply(lambda x: assign_label(x[7], x[10]), axis=1)\n",
    "            \n",
    "            plot_graphs(df, model_name, model_path, session, task, dp)\n",
    "             \n",
    "            summary = summarize_dataframe(df, i, 0)\n",
    "            task_summaries.append(summary)\n",
    "            for k in total_summary:\n",
    "                if k in [\"trial_num\", \"isCatch\"]:\n",
    "                    continue\n",
    "                total_summary[k] += summary[k].iloc[0]\n",
    "        if len(data_paths):\n",
    "            for k in total_summary:\n",
    "                if k in [\"trial_num\", \"isCatch\", \"num_crashes\"]:\n",
    "                    continue\n",
    "                total_summary[k] /= len(data_paths)\n",
    "\n",
    "\n",
    "            print(f\"        {len(task_summaries)}\")\n",
    "            task_summaries.append(pd.DataFrame([total_summary]))\n",
    "            summary_df = pd.concat(task_summaries)\n",
    "\n",
    "            summary_df.to_csv(f\"{model_path}/{task}_summary.csv\", index=False)\n",
    "\n",
    "            summary_df[\"task\"] = summary_df[\"trial_num\"].apply(lambda _: task)\n",
    "\n",
    "            model_summaries.append(summary_df[summary_df[\"trial_num\"] == \"summary\"].drop(columns=[\"trial_num\", \"isCatch\"]))\n",
    "    \n",
    "    if len(model_summaries):\n",
    "        print(f\"    {len(model_summaries)}\")\n",
    "        model_summary_df = pd.concat(model_summaries)\n",
    "        model_summary_df = model_summary_df[COLUMNS]\n",
    "        \n",
    "        model_summary_df.to_csv(f\"{model_path}/{session}_summary.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../output/ijcai_models/\"\n",
    "model_data_paths = glob.glob(f\"{root_path}*\")\n",
    "model_data_paths.sort(key=lambda x: x.split(\"/\")[-1].split(\"_\")[-1])\n",
    "model_names = [x.split('/')[-1].upper() for x in model_data_paths]\n",
    "model_data_paths = list(zip(model_names, model_data_paths))\n",
    "\n",
    "for model_name, model_dp in model_data_paths:\n",
    "    print(model_name, model_dp)\n",
    "    process_data(model_name, model_dp, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COLUMNS =  [ 'perc_destab_actions', 'perc_anticip_actions', 'num_crashes',\n",
    "       'average_crash_prob', 'crash_freqs', 'mean_dist_dob', 'ang_pos_sd',\n",
    "       'vel_mag_mean', 'ang_vel_sd', 'vel_rms', 'defl_mag_mean',\n",
    "       'avg_dur_cont_non_zero_inputs', 'avg_dur_cont_zero_inputs',\n",
    "        ]\n",
    "\n",
    "COLUMNS_PLOT = ['num_crashes', 'perc_destab_actions','mean_dist_dob', 'ang_pos_sd', 'vel_mag_mean', 'vel_rms']\n",
    "total_diff_data = []\n",
    "\n",
    "\n",
    "for ppt_id, ppt_path in model_data_paths:\n",
    "    print(ppt_id, ppt_path)\n",
    "    \n",
    "    original = pd.read_csv(f\"{ppt_path}/original_summary.csv\")\n",
    "    retrained = pd.read_csv(f\"{ppt_path}/retrained_summary.csv\")\n",
    "    \n",
    "    for col in COLUMNS_PLOT:\n",
    "        diff = retrained[retrained.trial_num == \"summary\"][col].iloc[0] - original[original.trial_num == \"summary\"][col].iloc[0]\n",
    "\n",
    "        if diff == np.inf:\n",
    "            diff = 0\n",
    "        total_diff_data.append(\n",
    "            [ppt_id, ppt_id, columns_shortHand[col], diff, \"diff\"]\n",
    "        )\n",
    "\n",
    "total_diff_df = pd.DataFrame(total_diff_data, columns=[\"ppt\", \"model\", \"feature\", \"diff\", \"version\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.swarmplot(x=\"feature\", y=\"diff\",\n",
    "            hue=\"model\",\n",
    "            data=total_diff_df, hue_order=[assistant_map[x] for x in assistant_map],\n",
    "            size=10\n",
    "            )\n",
    "\n",
    "plt.xlabel(\"Task metrics\", labelpad=20)\n",
    "plt.ylabel(\"Absolute difference\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig(f\"./results/human_ai_study/difference_bw_original_retrained_models.pdf\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "human_study_data_path = \"../output/human_ai_study/\"\n",
    "\n",
    "participants_models_mappings = pd.read_excel(\"../python_vip/participants_models_mappings.xlsx\")\n",
    "\n",
    "model_information = {}\n",
    "\n",
    "with open(\"../python_vip/name_mappings.json\") as f:\n",
    "    model_information = json.load(f)\n",
    "\n",
    "model_information\n",
    "\n",
    "participant_data_paths = glob.glob(f\"{human_study_data_path}*\")\n",
    "participant_ids = sorted([x.split('/')[-1] for x in participant_data_paths], \n",
    "                         key=lambda x: x.split('_')[-1])\n",
    "# participant_ids\n",
    "\n",
    "model_ppt_groups = {}\n",
    "\n",
    "for model in model_information['models']:\n",
    "    model_name = model_information['models'][model]['name']\n",
    "    model_ppt_groups[model_name] = {}\n",
    "    model_ppt_groups[model_name]['ppts']  = participants_models_mappings[participants_models_mappings['model'] == model].dropna()['participant'].to_list()[:4]\n",
    "\n",
    "# model_ppt_groups\n",
    "\n",
    "d = []\n",
    "\n",
    "for model in model_ppt_groups:\n",
    "    print(model)\n",
    "    dis_episodes_per_model = []\n",
    "\n",
    "    for x in model_ppt_groups[model]['ppts']:\n",
    "        paths = glob.glob(f\"{human_study_data_path}{x}/{tasks['1st_session'][3]}/*/disagreement_episodes.txt\")\n",
    "        # print(x, paths)\n",
    "        t = []\n",
    "        for p in paths:\n",
    "            dis_episodes = np.loadtxt(p, delimiter=',')\n",
    "\n",
    "            t.append(len(dis_episodes))\n",
    "        d.append([model, x, np.average(t)])\n",
    "\n",
    "d = pd.DataFrame(d, columns=[\"model\", \"ppt\", \"average\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby(\"model\").describe().to_csv(\"./results/human_ai_study/HITL_disagreement_episodes_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_shorthand = {\n",
    "    \"rdk_alone_AI_assist_first\": \"RDK Assist 1\", \"rdk_alone_AI_assist_second_original\": \"RDK Assist 2 - Original\", \"rdk_alone_AI_assist_second_retrained\": \"RDK Assist 2 - Retrained\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_helpful_suggestions(df: pd.DataFrame):\n",
    "\n",
    "    count_followed = 0\n",
    "    count_recovered = 0\n",
    "    count_crashed = 0\n",
    "    count_provided_suggestions = 0\n",
    "    time_to_recover = []\n",
    "    time_to_crash = []\n",
    "\n",
    "    suggestion_started = False\n",
    "    start_time = 0\n",
    "    start_pos = 0\n",
    "    curr_time = 0\n",
    "    suggestion_direction = 0\n",
    "    start_index = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if not suggestion_started:\n",
    "            suggestion_started = df.iloc[i][\"action_made_by\"] == 4 and abs(df.iloc[i][\"angular position\"]) > np.degrees(0.2)\n",
    "            if suggestion_started:\n",
    "                start_time = df.iloc[i][\"time\"]\n",
    "                curr_time = start_time\n",
    "                start_pos = df.iloc[i][\"angular position\"]\n",
    "                start_index = i\n",
    "                suggestion_direction = np.sign(df.iloc[i][\"assistant_actions\"])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # check position: if returned to safe region, update counts elif still not safe, continue, if crashed update counts and continue\n",
    "            \n",
    "            if curr_time - df.iloc[i][\"time\"] <= 0.45:\n",
    "                if suggestion_direction == np.sign(df.iloc[i][\"pilot_actions\"]):\n",
    "                    count_followed += 1\n",
    "            else:\n",
    "                start_index += 1\n",
    "                curr_time = df.iloc[start_index][\"time\"]\n",
    "                suggestion_direction = np.sign(df.iloc[start_index][\"assistant_actions\"])\n",
    "\n",
    "            \n",
    "            \n",
    "            pos = df.iloc[i][\"angular position\"]\n",
    "            recovered = abs(pos) < np.degrees(0.2) and abs(pos) < abs(start_pos)\n",
    "            crashed = abs(pos) >= 60\n",
    "            if recovered:\n",
    "                suggestion_started = False\n",
    "                time_to_recover.append(df.iloc[i][\"time\"] - start_time)\n",
    "                count_recovered += 1\n",
    "            elif crashed :\n",
    "                suggestion_started = False\n",
    "                time_to_crash.append(df.iloc[i][\"time\"] - start_time)\n",
    "                count_crashed += 1\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    if len(time_to_crash) == 0:\n",
    "        time_to_crash.append(0)\n",
    "    if len(time_to_recover) == 0:\n",
    "        time_to_recover.append(0)\n",
    "\n",
    "    count_provided_suggestions = len(df[df[\"action_made_by\"] == 4])\n",
    "    return (\n",
    "        count_recovered,\n",
    "        np.average(time_to_recover),\n",
    "        count_crashed,\n",
    "        np.average(time_to_crash),\n",
    "        count_followed,\n",
    "        count_provided_suggestions\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = []\n",
    "output1_catch = []\n",
    "output2 = []\n",
    "output2_catch = []\n",
    "for ppt_id, ppt_path in ppt_data_paths:\n",
    "    print(ppt_id, ppt_path)\n",
    "    \n",
    "    for task in [\"rdk_alone_AI_assist_first\", \"rdk_alone_AI_assist_second_original\", \"rdk_alone_AI_assist_second_retrained\"]:\n",
    "        print(f\"    {task}\")\n",
    "        curr_path = f\"{ppt_path}/{task}/\"\n",
    "        data_paths = glob.glob(f\"{curr_path}*/*.csv\")\n",
    "        catch_trial = glob.glob(f\"{curr_path}*/*.json\")\n",
    "        toSkip = set()\n",
    "        if len(catch_trial):\n",
    "            subpath = \"/\".join(catch_trial[0].split('/')[:-1])\n",
    "            for p in data_paths:\n",
    "                if subpath in p:\n",
    "                    toSkip.add(p)\n",
    "        elif task == \"rdk_alone_AI_assist_first\":\n",
    "            toSkip.add(data_paths[0])\n",
    "\n",
    "        # print(len(toSkip))\n",
    "        temp1 = []\n",
    "        temp2 = []\n",
    "        model_used = participant_model_mappings[participant_model_mappings.participant == ppt_id][\"model\" if task==\"rdk_alone_AI_assist_first\" else \"model_second_session\"].iloc[0]\n",
    "        for i, dp in enumerate(data_paths):\n",
    "            df = pd.read_csv(dp)\n",
    "            count_recov, time_recov, count_crash, time_crash, count_followed, count_provided = num_helpful_suggestions(df)\n",
    "            # print(count_followed)\n",
    "            if dp not in toSkip:\n",
    "                temp1.append(len(df[df.action_made_by == 4])/len(df))\n",
    "                temp2.append([count_recov, time_recov, count_crash, time_crash, count_followed, len(df), count_provided])\n",
    "            else:\n",
    "                output1_catch.append([ppt_id, assistant_map[model_used], len(df[df.action_made_by == 4])/len(df), task_shorthand[task]])\n",
    "\n",
    "                output2_catch.append([\n",
    "                    ppt_id, assistant_map[model_used], task_shorthand[task], \n",
    "                    count_recov, time_recov, count_crash, time_crash, count_followed, len(df), count_provided\n",
    "                ])\n",
    "\n",
    "            # break\n",
    "        # break\n",
    "        temp2 = np.array(temp2)\n",
    "        # print(temp2)\n",
    "        output1.append([ppt_id, assistant_map[model_used], np.average(temp1), task_shorthand[task]])\n",
    "        output2.append([ppt_id, assistant_map[model_used], task_shorthand[task],\n",
    "                        np.sum(temp2[:, 0]), np.average(temp2[:, 1]), np.sum(temp2[:, 2]), np.average(temp2[:, 3]), np.sum(temp2[:, 4]), np.sum(temp2[:, 5]), np.sum(temp2[:, 6]),\n",
    "                        ])\n",
    "\n",
    "prop_suggestions_given = pd.DataFrame(output1, columns=[\"ppt\", \"model\", \"proportion\", \"task\"])\n",
    "prop_suggestions_given_catch = pd.DataFrame(output1_catch, columns=[\"ppt\", \"model\", \"proportion\", \"task\"])\n",
    "\n",
    "recovery_crash_stats = pd.DataFrame(output2, columns=[\"ppt\", \"model\", \"task\", \"count_recovered\", \"time_recovered\", \"count_crashed\", \"time_crashed\", \"count_followed\", \"length_trials\", \"count_provided_suggestions\"])\n",
    "recovery_crash_stats_catch = pd.DataFrame(output2_catch, columns=[\"ppt\", \"model\", \"task\", \"count_recovered\", \"time_recovered\", \"count_crashed\", \"time_crashed\", \"count_followed\", \"length_trials\", \"count_provided_suggestions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_crash_stats.to_csv(\"./results/human_ai_study/recovery_crash_stats.csv\", index=False)\n",
    "recovery_crash_stats_catch.to_csv(\"./results/human_ai_study/recovery_crash_stats_catch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"task\", y=\"proportion\",\n",
    "            hue=\"model\",\n",
    "            data=prop_suggestions_given, fliersize=7)\n",
    "\n",
    "plt.xlabel(\"Task\", labelpad=20)\n",
    "plt.ylabel(\"Proportion of trial\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig(f\"./results/human_ai_study/proportion_trial_given_suggestions.pdf\", dpi=300, bbox_inches=\"tight\");\n",
    "# sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set(rc = {'figure.figsize':(1, 7),},)\n",
    "# sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_theme(style=\"ticks\", font_scale=2.5, palette=\"bright\", context=\"paper\" )\n",
    "\n",
    "# Load the example tips dataset\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# print(tips.head())\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"task\", y=\"proportion\",\n",
    "            hue=\"model\",\n",
    "            data=prop_suggestions_given_catch, fliersize=7)\n",
    "\n",
    "plt.xlabel(\"Task\", labelpad=20)\n",
    "plt.ylabel(\"Proportion of trial\");\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig(f\"./results/human_ai_study/proportion_trial_given_suggestions_catch.pdf\", dpi=300, bbox_inches=\"tight\");\n",
    "# sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikhils-environment-m3x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
