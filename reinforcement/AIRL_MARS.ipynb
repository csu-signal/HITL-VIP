{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b4435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imitation.data.types import Transitions\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(\"MARS_AIRL_expert_transitions\", \"rb\")\n",
    "demos = pickle.load(file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40faec7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b03c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import SAC, A2C\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import gym\n",
    "from envs.pendulumV21 import PendulumEnv as pendulum\n",
    "\n",
    "\n",
    "env = pendulum()\n",
    "venv = make_vec_env(pendulum, n_envs=8)\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "learner = SAC(\"MlpPolicy\", venv, action_noise=action_noise, verbose=1, device='cpu')\n",
    "# learner = A2C(\"MlpPolicy\", venv, verbose=1)#, device='cpu')\n",
    "reward_net = BasicShapedRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31489888",
   "metadata": {},
   "outputs": [],
   "source": [
    "airl_trainer = AIRL(\n",
    "    demonstrations=demos,\n",
    "    demo_batch_size=64,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    allow_variable_horizon=True\n",
    ")\n",
    "\n",
    "# learner_rewards_before_training, _ = evaluate_policy(\n",
    "#      learner, venv, 1, return_episode_rewards=True\n",
    "# )\n",
    "airl_trainer.train(300000)  # Note: set to 300000 for better results\n",
    "# learner_rewards_after_training, _ = evaluate_policy(\n",
    "#     learner, venv, 100, return_episode_rewards=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"./saved_models/sac_airl_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348d85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
