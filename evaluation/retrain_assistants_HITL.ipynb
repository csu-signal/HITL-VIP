{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_study_data_path = \"../output/human_ai_study/\"\n",
    "\n",
    "participants_models_mappings = pd.read_excel(\"../python_vip/participants_models_mappings.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_models_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_information = {}\n",
    "\n",
    "with open(\"../python_vip/name_mappings.json\") as f:\n",
    "    model_information = json.load(f)\n",
    "\n",
    "model_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"session_1\": {\n",
    "        \"pendulum\": \"pendulum\", \"rdk_alone\": \"rdk_alone_first\", \n",
    "        \"rdk_AI_assist\": \"rdk_alone_AI_assist_first\", \"rdk_hitl\": \"rdk_alone_hitl\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting disagreement episodes from HITL task trials. \n",
    "\n",
    "get participant ids\n",
    "\n",
    "sort them based on Number\n",
    "\n",
    "group them based on the assistant AI\n",
    "\n",
    "get disagreement episodes\n",
    "\n",
    "compile and save them for each AI - done\n",
    "\n",
    "train each AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_paths = glob.glob(f\"{human_study_data_path}*\")\n",
    "participant_ids = sorted([x.split('/')[-1] for x in participant_data_paths], \n",
    "                         key=lambda x: x.split('_')[-1])\n",
    "participant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_models_mappings.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppt_groups = {}\n",
    "\n",
    "for model in model_information['models']:\n",
    "    model_name = model_information['models'][model]['name']\n",
    "    model_ppt_groups[model_name] = {}\n",
    "    model_ppt_groups[model_name]['ppts']  = participants_models_mappings[participants_models_mappings['model'] == model].dropna()['participant'].to_list()[:4]\n",
    "\n",
    "model_ppt_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_ppt_groups:\n",
    "    print(model)\n",
    "    dis_episodes_per_model = []\n",
    "\n",
    "    for x in model_ppt_groups[model]['ppts']:\n",
    "        dis_episodes_per_model.extend(glob.glob(f\"{human_study_data_path}{x}/{tasks['session_1']['rdk_hitl']}/*/disagreement_episodes.txt\"))\n",
    "    \n",
    "    print(dis_episodes_per_model)\n",
    "    # model_ppt_groups[model]['paths'] = dis_episodes_per_model\n",
    "\n",
    "    temp_data = []\n",
    "    for pth in dis_episodes_per_model:\n",
    "        print(pth)\n",
    "        print(\"here\")\n",
    "        dis_episodes = np.loadtxt(pth, delimiter=',')\n",
    "        \n",
    "        if len(dis_episodes): \n",
    "            if dis_episodes.ndim != 2: dis_episodes = dis_episodes.reshape(-1, dis_episodes.size)\n",
    "            temp_data.append(dis_episodes)\n",
    "        else:\n",
    "            print(len(dis_episodes))\n",
    "    try:\n",
    "        model_ppt_groups[model]['disagreement_data'] = np.concatenate(temp_data, axis=0)\n",
    "    except Exception as e:\n",
    "        print(temp_data)\n",
    "        print(e)\n",
    "        for i in range(len(temp_data)):\n",
    "            print(i, temp_data[i].shape)\n",
    "        print(temp_data[3].reshape(1, -1).shape)\n",
    "        # print(temp_data)\n",
    "\n",
    "    print(model_ppt_groups[model]['disagreement_data'].shape)\n",
    "\n",
    "    np.savetxt(f\"../data/human_ai_study/{model}_disagreement_episodes.txt\", model_ppt_groups[model]['disagreement_data'],  delimiter=',')\n",
    "# model_ppt_groups\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppt_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retraining vanilla SAC model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sac_penalizeDeflection\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DDPG\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "from reinforcement.envs.pendulum_penalizeDeflection import PendulumEnv as penalizeDeflection\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "def pretrain_agent(\n",
    "    student,\n",
    "    env,\n",
    "    train_expert_dataset, test_expert_dataset,\n",
    "    trace=[],\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                trace.append([epoch, loss.item()])\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    dist = model.get_distribution(data)\n",
    "                    action_prediction = dist.distribution.logits\n",
    "                    target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    student.policy = model\n",
    "\n",
    "    return student\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"sac_penalizeDeflection\"\n",
    "model_id = \"A\"\n",
    "\n",
    "model_dis_episodes = model_ppt_groups[model_name]['disagreement_data']\n",
    "\n",
    "features, model_actions, human_actions = model_dis_episodes[:, :-2], model_dis_episodes[:, -2], model_dis_episodes[:, -1]\n",
    "\n",
    "targets = model_actions * -1\n",
    "targets = targets.reshape(-1, 1)\n",
    "\n",
    "expert_observations = features\n",
    "expert_actions = targets\n",
    "\n",
    "train_config = {\n",
    "    \"model_type\": \"sac\"\n",
    "}\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "student_model = None\n",
    "\n",
    "if train_config['model_type'] == 'ddpg':\n",
    "    student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "else:\n",
    "    student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "\n",
    "# env = student_model.get_env()\n",
    "\n",
    "train_config = {\n",
    "    \"model_type\": \"sac\"\n",
    "}\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "student_model = None\n",
    "\n",
    "\n",
    "\n",
    "# env = student_model.get_env()\n",
    "print(env)\n",
    "print(student_model)\n",
    "\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.9 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "len(train_expert_dataset), len(test_expert_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# epochs=100\n",
    "\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "for epochs, lr in [(100, 1e-3), (100, 1e-4), (100, 1e-5), (200, 1e-3), (200, 1e-4), (200, 1e-5),]:\n",
    "    trace = []\n",
    "    env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "    if train_config['model_type'] == 'ddpg':\n",
    "        student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "    else:\n",
    "        student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "    # env.\n",
    "    updated_model = pretrain_agent(\n",
    "        student_model,\n",
    "        env,\n",
    "        train_expert_dataset, test_expert_dataset,\n",
    "        trace=trace,\n",
    "        epochs=epochs,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=lr,\n",
    "        log_interval=100,\n",
    "        no_cuda=False,\n",
    "        seed=1,\n",
    "        batch_size=64,\n",
    "        test_batch_size=10,\n",
    "    )\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(updated_model, env, n_eval_episodes=5)\n",
    "    print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    res.append([mean_reward, std_reward, epochs, lr])\n",
    "\n",
    "\n",
    "res\n",
    "    # break\n",
    "\n",
    "if train_config['model_type'] == 'ddpg':\n",
    "    student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "else:\n",
    "    student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(student_model, env, n_eval_episodes=5)\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "res.append([mean_reward, std_reward, None, None])\n",
    "\n",
    "res = pd.DataFrame(res, columns=[\"mean_reward\", \"std_reward\", \"epochs\", \"lr\"])\n",
    "res = res.sort_values([\"mean_reward\", \"std_reward\"], ascending=False)\n",
    "\n",
    "epochs, lr = None, None\n",
    "if res.iloc[0].epochs != None:\n",
    "    epochs, lr = int(res.iloc[0].epochs), res.iloc[0].lr\n",
    "else:\n",
    "    epochs, lr = int(res.iloc[1].epochs), res.iloc[1].lr\n",
    "\n",
    "updated_model = pretrain_agent(\n",
    "        student_model,\n",
    "        env,\n",
    "        train_expert_dataset, test_expert_dataset,\n",
    "        trace=trace,\n",
    "        epochs=epochs,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=lr,\n",
    "        log_interval=100,\n",
    "        no_cuda=False,\n",
    "        seed=1,\n",
    "        batch_size=64,\n",
    "        test_batch_size=10,\n",
    "    )\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(updated_model, env, n_eval_episodes=5)\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "updated_model.save(f\"../working_models/assistants/{model_name}_alpha_study_batch_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retraining vanilla DDPG model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ddpg_penalizeDeflection\n",
    "\n",
    "model_name = \"ddpg_penalizeDeflection\"\n",
    "model_id = \"C\"\n",
    "\n",
    "model_dis_episodes = model_ppt_groups[model_name]['disagreement_data']\n",
    "\n",
    "features, model_actions, human_actions = model_dis_episodes[:, :-2], model_dis_episodes[:, -2], model_dis_episodes[:, -1]\n",
    "\n",
    "targets = model_actions * -1\n",
    "targets = targets.reshape(-1, 1)\n",
    "\n",
    "expert_observations = features\n",
    "expert_actions = targets\n",
    "\n",
    "train_config = {\n",
    "    \"model_type\": \"ddpg\"\n",
    "}\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "student_model = None\n",
    "\n",
    "if train_config['model_type'] == 'ddpg':\n",
    "    student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "else:\n",
    "    student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "\n",
    "# env = student_model.get_env()\n",
    "\n",
    "train_config = {\n",
    "    \"model_type\": \"ddpg\"\n",
    "}\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "student_model = None\n",
    "\n",
    "\n",
    "\n",
    "# env = student_model.get_env()\n",
    "print(env)\n",
    "print(student_model)\n",
    "\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.9 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "len(train_expert_dataset), len(test_expert_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# epochs=100\n",
    "\n",
    "res = []\n",
    "\n",
    "\n",
    "\n",
    "for epochs, lr in [(100, 1e-3), (100, 1e-4), (100, 1e-5), (200, 1e-3), (200, 1e-4), (200, 1e-5),]:\n",
    "    trace = []\n",
    "    env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "    if train_config['model_type'] == 'ddpg':\n",
    "        student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "    else:\n",
    "        student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "    # env.\n",
    "    updated_model = pretrain_agent(\n",
    "        student_model,\n",
    "        env,\n",
    "        train_expert_dataset, test_expert_dataset,\n",
    "        trace=trace,\n",
    "        epochs=epochs,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=lr,\n",
    "        log_interval=100,\n",
    "        no_cuda=False,\n",
    "        seed=1,\n",
    "        batch_size=64,\n",
    "        test_batch_size=10,\n",
    "    )\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(updated_model, env, n_eval_episodes=5)\n",
    "    print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    res.append([mean_reward, std_reward, epochs, lr])\n",
    "\n",
    "\n",
    "res\n",
    "    # break\n",
    "\n",
    "if train_config['model_type'] == 'ddpg':\n",
    "    student_model = DDPG.load(model_information[\"models\"][model_id][\"path\"])\n",
    "else:\n",
    "    student_model = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(student_model, env, n_eval_episodes=5)\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "res.append([mean_reward, std_reward, None, None])\n",
    "\n",
    "res = pd.DataFrame(res, columns=[\"mean_reward\", \"std_reward\", \"epochs\", \"lr\"])\n",
    "res = res.sort_values([\"mean_reward\", \"std_reward\"], ascending=False)\n",
    "print(res)\n",
    "epochs, lr = None, None\n",
    "if not np.isnan(res.iloc[0].epochs):\n",
    "    epochs, lr = int(res.iloc[0].epochs), res.iloc[0].lr\n",
    "else:\n",
    "    epochs, lr = int(res.iloc[1].epochs), res.iloc[1].lr\n",
    "\n",
    "updated_model = pretrain_agent(\n",
    "        student_model,\n",
    "        env,\n",
    "        train_expert_dataset, test_expert_dataset,\n",
    "        trace=trace,\n",
    "        epochs=epochs,\n",
    "        scheduler_gamma=0.7,\n",
    "        learning_rate=lr,\n",
    "        log_interval=100,\n",
    "        no_cuda=False,\n",
    "        seed=1,\n",
    "        batch_size=64,\n",
    "        test_batch_size=10,\n",
    "    )\n",
    "\n",
    "env = penalizeDeflection(render_mode='human')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(updated_model, env, n_eval_episodes=5)\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "updated_model.save(f\"../working_models/assistants/{model_name}_alpha_study_batch_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retraining MLP all prof assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_learning.utils.utils import *\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from python_vip.vip import load_model, print_model_summary\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrainDataset(Dataset):\n",
    "    def __init__(self, X, Y) -> None:\n",
    "        super().__init__()\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.Y = torch.from_numpy(Y)\n",
    "        else:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].float(), self.Y[idx].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "balancing_data_path = f\"{data_path}data_all_radians.csv\"\n",
    "proficiencies_data_path = f\"{data_path}overall_proficiency.csv\"\n",
    "print(\"Data file used: \", balancing_data_path)\n",
    "print(\"Proficiency file used: \", proficiencies_data_path)\n",
    "\n",
    "orig_data = pd.read_csv(balancing_data_path)\n",
    "orig_data['joystickX'] = -1 * orig_data['joystickX']\n",
    "\n",
    "proficiencies = pd.read_csv(proficiencies_data_path)\n",
    "ppts, ppt_data = filter_data_based_proficiency(orig_data, proficiencies, proficiency=[\"Good\"])\n",
    "train_data, test_data = train_test_split_trials(ppt_data, ppts, train=0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ml_dataset = MarsDataset(train_data, past_window_size=0, future_time=1, lstm_format=False)\n",
    "\n",
    "test_ml_dataset = MarsDataset(test_data, past_window_size=0,future_time=1, lstm_format=False, Xmeans=train_ml_dataset.Xmeans, Xstds=train_ml_dataset.Xstds, Tmeans=train_ml_dataset.Tmeans, Tstds=train_ml_dataset.Tstds)\n",
    "\n",
    "test_ppt_trial = MarsDataset(orig_data[orig_data.peopleTrialKey == '2_ms_P2/20_600back_Block5_trial_020.csv'], past_window_size=0, future_time=1, lstm_format=False, Xmeans=train_ml_dataset.Xmeans, Xstds=train_ml_dataset.Xstds, Tmeans=train_ml_dataset.Tmeans, Tstds=train_ml_dataset.Tstds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"assistant_mars_mlp_all_prof\"\n",
    "model_id = \"D\"\n",
    "\n",
    "config = model_information[\"models\"][model_id]\n",
    "\n",
    "epochs_lrs = [(50, 1e-3), (50, 1e-4), (50, 1e-5), (100, 1e-3), (100, 1e-4), (100, 1e-5),]\n",
    "\n",
    "model_dis_episodes = model_ppt_groups[model_name]['disagreement_data']\n",
    "\n",
    "features, model_actions, human_actions = model_dis_episodes[:, :-2], model_dis_episodes[:, -2], model_dis_episodes[:, -1]\n",
    "\n",
    "targets = -1 * model_actions.reshape(-1, 1)\n",
    "\n",
    "features = torch.from_numpy(features).float()\n",
    "targets = torch.from_numpy(targets).float()\n",
    "\n",
    "trainX_hitl, testX_hitl, trainY_hitl, testY_hitl = train_test_split(features, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader, test_loader = None, None\n",
    "\n",
    "trainX = (trainX_hitl - train_ml_dataset.Xmeans) / train_ml_dataset.Xstds\n",
    "trainY = (trainY_hitl - train_ml_dataset.Tmeans) / train_ml_dataset.Tstds\n",
    "\n",
    "testX = (testX_hitl - train_ml_dataset.Xmeans) / train_ml_dataset.Xstds\n",
    "testY = (testY_hitl - train_ml_dataset.Tmeans) / train_ml_dataset.Tstds\n",
    "\n",
    "if config['type'] in {'lstm', 'gru', 'rnn'}:\n",
    "    trainX = trainX.reshape(-1, config['past_window_size']+1, 3)\n",
    "    testX = testX.reshape(-1, config['past_window_size']+1, 3)\n",
    "\n",
    "train_set = RetrainDataset(trainX, trainY)\n",
    "test_set = RetrainDataset(testX, testY)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=10)\n",
    "test_loader = DataLoader(test_set, batch_size=16, num_workers=10)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for epochs, lr in epochs_lrs:\n",
    "    model = load_model(config[\"path\"], config['type'])\n",
    "    model.lr = lr\n",
    "    name = f\"{config['name']}_hitl_retrain_epochs_{epochs}_lr_{lr}\"\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            monitor='val_MAE',\n",
    "                            dirpath=f\"./output/{name}/\",\n",
    "                            filename='best_checkpoint',\n",
    "                            save_top_k=1,\n",
    "                            mode='min',\n",
    "                        )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_MAE',\n",
    "        min_delta=0.00001,\n",
    "        patience= 5,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger('lightning_logs', name=f\"{name}\")\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        precision=32,\n",
    "        accelerator=\"auto\",\n",
    "        num_sanity_val_steps=0,\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks=[checkpoint_callback, early_stopping],\n",
    "        logger=logger,\n",
    "        devices=[0]\n",
    "        # strategy='ddp'\n",
    "    )\n",
    "\n",
    "    # trainer.\n",
    "    trainer.fit(model, train_loader, test_loader) \n",
    "\n",
    "    testX = testX_hitl\n",
    "    testY = testY_hitl\n",
    "\n",
    "    # human_actions = disagreement_data[:, -1]\n",
    "    if config['type'] in {'lstm', 'gru', 'rnn'}:\n",
    "        testX = testX.reshape(-1, config['past_window_size']+1, 3)\n",
    "        \n",
    "    pred_joys = model.use(testX)\n",
    "    # dl_seconds = np.float32(test_ppt_dataset.metas[:,0])\n",
    "    actual_joys = testY\n",
    "\n",
    "\n",
    "    mae_test_hitl = mean_absolute_error(actual_joys, pred_joys),\n",
    "    max_err_test_hitl = max_error(actual_joys, pred_joys)\n",
    "    rmse_test_hitl = mean_squared_error(actual_joys, pred_joys, squared=False)\n",
    "\n",
    "    features = test_ppt_trial.features_physics.float()\n",
    "                # print(features.shape)\n",
    "    Xs = features.reshape(-1,config['past_window_size']+1, 3) if config['type'] != 'mlp' else features\n",
    "    pred_joys = model.use(Xs)\n",
    "    # dl_seconds = np.float32(test_ppt_dataset.metas[:,0])\n",
    "    actual_joys = test_ppt_trial.targets.numpy()\n",
    "\n",
    "\n",
    "    mae_test_ppt = mean_absolute_error(actual_joys, pred_joys),\n",
    "    max_err_test_ppt = max_error(actual_joys, pred_joys)\n",
    "    rmse_test_ppt = mean_squared_error(actual_joys, pred_joys, squared=False)\n",
    "\n",
    "    results.append([epochs, lr, mae_test_ppt[0], rmse_test_ppt, max_err_test_ppt, mae_test_hitl[0], rmse_test_hitl, max_err_test_hitl])\n",
    "\n",
    "\n",
    "\n",
    "res = pd.DataFrame(results, columns=['epochs', 'lr', 'mae_trial', 'rmse_trial', 'max_err_trial', 'mae_hitl', 'rmse_hitl', 'max_err_hitl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ml_dataset = MarsDataset(train_data, past_window_size=10, future_time=1, lstm_format=True)\n",
    "\n",
    "test_ml_dataset = MarsDataset(test_data, past_window_size=10,future_time=1, lstm_format=True, Xmeans=train_ml_dataset.Xmeans, Xstds=train_ml_dataset.Xstds, Tmeans=train_ml_dataset.Tmeans, Tstds=train_ml_dataset.Tstds)\n",
    "\n",
    "test_ppt_trial = MarsDataset(orig_data[orig_data.peopleTrialKey == '2_ms_P2/20_600back_Block5_trial_020.csv'], past_window_size=10, future_time=1, lstm_format=True, Xmeans=train_ml_dataset.Xmeans, Xstds=train_ml_dataset.Xstds, Tmeans=train_ml_dataset.Tmeans, Tstds=train_ml_dataset.Tstds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"assistant_mars_lstm_good_small_window\"\n",
    "model_id = \"E\"\n",
    "\n",
    "config = model_information[\"models\"][model_id]\n",
    "\n",
    "epochs_lrs = [(50, 1e-3), (50, 1e-4), (50, 1e-5), (100, 1e-3), (100, 1e-4), (100, 1e-5),]\n",
    "\n",
    "model_dis_episodes = model_ppt_groups[model_name]['disagreement_data']\n",
    "\n",
    "features, model_actions, human_actions = model_dis_episodes[:, :-2], model_dis_episodes[:, -2], model_dis_episodes[:, -1]\n",
    "\n",
    "targets = -1 * model_actions.reshape(-1, 1)\n",
    "\n",
    "features = torch.from_numpy(features).float()\n",
    "targets = torch.from_numpy(targets).float()\n",
    "\n",
    "trainX_hitl, testX_hitl, trainY_hitl, testY_hitl = train_test_split(features, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader, test_loader = None, None\n",
    "print(trainX_hitl.shape, train_ml_dataset.Xmeans.shape, train_ml_dataset.Xstds.shape)\n",
    "trainX = (trainX_hitl - train_ml_dataset.Xmeans) / train_ml_dataset.Xstds\n",
    "trainY = (trainY_hitl - train_ml_dataset.Tmeans) / train_ml_dataset.Tstds\n",
    "\n",
    "testX = (testX_hitl - train_ml_dataset.Xmeans) / train_ml_dataset.Xstds\n",
    "testY = (testY_hitl - train_ml_dataset.Tmeans) / train_ml_dataset.Tstds\n",
    "\n",
    "if config['type'] in {'lstm', 'gru', 'rnn'}:\n",
    "    trainX = trainX.reshape(-1, train_ml_dataset.Xstds.shape[0]//3, 3)\n",
    "    testX = testX.reshape(-1, train_ml_dataset.Xstds.shape[0]//3, 3)\n",
    "\n",
    "train_set = RetrainDataset(trainX, trainY)\n",
    "test_set = RetrainDataset(testX, testY)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=10)\n",
    "test_loader = DataLoader(test_set, batch_size=16, num_workers=10)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for epochs, lr in epochs_lrs:\n",
    "    model = load_model(config[\"path\"], config['type'])\n",
    "    model.lr = lr\n",
    "    name = f\"{config['name']}_hitl_retrain_epochs_{epochs}_lr_{lr}\"\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            monitor='val_MAE',\n",
    "                            dirpath=f\"./output/{name}/\",\n",
    "                            filename='best_checkpoint',\n",
    "                            save_top_k=1,\n",
    "                            mode='min',\n",
    "                        )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_MAE',\n",
    "        min_delta=0.00001,\n",
    "        patience= 5,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger('lightning_logs', name=f\"{name}\")\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        precision=32,\n",
    "        accelerator=\"auto\",\n",
    "        num_sanity_val_steps=0,\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks=[checkpoint_callback, early_stopping],\n",
    "        logger=logger,\n",
    "        devices=[0]\n",
    "        # strategy='ddp'\n",
    "    )\n",
    "\n",
    "    # trainer.\n",
    "    trainer.fit(model, train_loader, test_loader) \n",
    "\n",
    "    testX = testX_hitl\n",
    "    testY = testY_hitl\n",
    "\n",
    "    # human_actions = disagreement_data[:, -1]\n",
    "    if config['type'] in {'lstm', 'gru', 'rnn'}:\n",
    "        testX = testX.reshape(-1, train_ml_dataset.Xstds.shape[0]//3, 3)\n",
    "        \n",
    "    pred_joys = model.use(testX)\n",
    "    # dl_seconds = np.float32(test_ppt_dataset.metas[:,0])\n",
    "    actual_joys = testY\n",
    "\n",
    "\n",
    "    mae_test_hitl = mean_absolute_error(actual_joys, pred_joys),\n",
    "    max_err_test_hitl = max_error(actual_joys, pred_joys)\n",
    "    rmse_test_hitl = mean_squared_error(actual_joys, pred_joys, squared=False)\n",
    "\n",
    "    features = test_ppt_trial.features_physics.float()\n",
    "                # print(features.shape)\n",
    "    Xs = features.reshape(-1,train_ml_dataset.Xstds.shape[0]//3, 3) if config['type'] != 'mlp' else features\n",
    "    pred_joys = model.use(Xs)\n",
    "    # dl_seconds = np.float32(test_ppt_dataset.metas[:,0])\n",
    "    actual_joys = test_ppt_trial.targets.numpy()\n",
    "\n",
    "\n",
    "    mae_test_ppt = mean_absolute_error(actual_joys, pred_joys),\n",
    "    max_err_test_ppt = max_error(actual_joys, pred_joys)\n",
    "    rmse_test_ppt = mean_squared_error(actual_joys, pred_joys, squared=False)\n",
    "\n",
    "    results.append([epochs, lr, mae_test_ppt[0], rmse_test_ppt, max_err_test_ppt, mae_test_hitl[0], rmse_test_hitl, max_err_test_hitl])\n",
    "\n",
    "\n",
    "\n",
    "res = pd.DataFrame(results, columns=['epochs', 'lr', 'mae_trial', 'rmse_trial', 'max_err_trial', 'mae_hitl', 'rmse_hitl', 'max_err_hitl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retraining AIRL SAC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sac_penalizeDeflection\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DDPG\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from imitation.data.types import Transitions\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "\n",
    "from reinforcement.envs.pendulumV21 import PendulumEnv\n",
    "\n",
    "\n",
    "model_name = \"sac_airl_300k\"\n",
    "model_id = \"B\"\n",
    "\n",
    "model_dis_episodes = model_ppt_groups[model_name]['disagreement_data']\n",
    "\n",
    "features, model_actions, human_actions = model_dis_episodes[:, :-2], model_dis_episodes[:, -2], model_dis_episodes[:, -1]\n",
    "\n",
    "targets = model_actions * -1\n",
    "targets = targets.reshape(-1, 1)\n",
    "\n",
    "\n",
    "env = PendulumEnv()\n",
    "# curr_observations = []\n",
    "next_observations = []\n",
    "infos = []\n",
    "dones = []\n",
    "\n",
    "for i in range(len(features)):\n",
    "    env.reset()\n",
    "    obs = features[i]\n",
    "    obs = [np.arctan2(obs[1], obs[0]), obs[2]]\n",
    "    # curr_observations.append(obs)\n",
    "    action = targets[i]\n",
    "    env.state = obs\n",
    "    next_obs, _, terminated, info = env.step(action)\n",
    "    # print(next_obs)\n",
    "    next_observations.append(next_obs)\n",
    "    infos.append(info)\n",
    "\n",
    "    dones.append(terminated)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(np.array(features).shape, np.array(targets).shape, np.array(infos).shape, np.array(next_observations).shape, np.array(dones).shape)\n",
    "\n",
    "demos = Transitions(\n",
    "    np.array(features, dtype=np.float64), np.array(targets, dtype=np.float64), np.array(infos), np.array(next_observations, dtype=np.float64), np.array(dones)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "print(env.action_space)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "res = []\n",
    "\n",
    "for epochs, lr in [(100, 1e-3), (100, 1e-4), (100, 1e-5), (200, 1e-3), (200, 1e-4), (200, 1e-5),]:\n",
    "    env = PendulumEnv()\n",
    "    venv = make_vec_env(PendulumEnv, n_envs=8)\n",
    "    \n",
    "    \n",
    "    learner = SAC.load(model_information[\"models\"][model_id][\"path\"])\n",
    "    learner.env = venv\n",
    "    learner.action_noise = action_noise\n",
    "    learner.learning_rate = lr\n",
    "\n",
    "    reward_net = BasicShapedRewardNet(\n",
    "        venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    "    )\n",
    "    # env.\n",
    "\n",
    "    airl_trainer = AIRL(\n",
    "        demonstrations=demos,\n",
    "        demo_batch_size=64,\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=venv,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net,\n",
    "        allow_variable_horizon=True\n",
    "    )\n",
    "\n",
    "    airl_trainer.train(epochs)\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(learner, env, n_eval_episodes=5)\n",
    "    print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    venv.close()\n",
    "    env.close()\n",
    "\n",
    "    res.append([mean_reward, std_reward, epochs, lr, learner])\n",
    "\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looked at the highest mean reward and saved the model manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0][-1].save(\"../working_models/assistants/sac_airl_300k_alpha_study_batch_1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikhils-environment-m3x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
